{"cells":[{"cell_type":"markdown","id":"c604170a","metadata":{"papermill":{"duration":0.015951,"end_time":"2021-11-09T00:09:34.522659","exception":false,"start_time":"2021-11-09T00:09:34.506708","status":"completed"},"tags":[],"id":"c604170a"},"source":["# Text Classification with SpaCy\n","\n","A common task in NLP is **text classification**. This is \"classification\" in the conventional machine learning sense, and it is applied to text. Examples include spam detection, sentiment analysis, and tagging customer queries. \n","\n","In this tutorial, you'll learn text classification with spaCy. The classifier will detect spam messages, a common functionality in most email clients. Here is an overview of the data you'll use:"]},{"cell_type":"markdown","source":["import libraries and read the data from csv file"],"metadata":{"id":"g-46Rx4tg9VC"},"id":"g-46Rx4tg9VC"},{"cell_type":"code","execution_count":null,"id":"799a3d4a","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:09:34.560082Z","iopub.status.busy":"2021-11-09T00:09:34.559101Z","iopub.status.idle":"2021-11-09T00:09:34.621970Z","shell.execute_reply":"2021-11-09T00:09:34.622502Z"},"papermill":{"duration":0.085805,"end_time":"2021-11-09T00:09:34.622786","exception":false,"start_time":"2021-11-09T00:09:34.536981","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"799a3d4a","executionInfo":{"status":"ok","timestamp":1681716574367,"user_tz":-420,"elapsed":1980,"user":{"displayName":"Tuấn Long Hồ","userId":"06884557049593639963"}},"outputId":"48eed02a-4221-4272-df37-814da3f9a1a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  label                                               text\n","0   ham  Go until jurong point, crazy.. Available only ...\n","1   ham                      Ok lar... Joking wif u oni...\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3   ham  U dun say so early hor... U c already then say...\n","4   ham  Nah I don't think he goes to usf, he lives aro...\n","5  spam  FreeMsg Hey there darling it's been 3 week's n...\n","6   ham  Even my brother is not like to speak with me. ...\n","7   ham  As per your request 'Melle Melle (Oru Minnamin...\n","8  spam  WINNER!! As a valued network customer you have...\n","9  spam  Had your mobile 11 months or more? U R entitle..."],"text/html":["\n","  <div id=\"df-53b0620b-dc2c-4b34-bca1-35f156a9e9dc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>spam</td>\n","      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ham</td>\n","      <td>Even my brother is not like to speak with me. ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ham</td>\n","      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>spam</td>\n","      <td>WINNER!! As a valued network customer you have...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>spam</td>\n","      <td>Had your mobile 11 months or more? U R entitle...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53b0620b-dc2c-4b34-bca1-35f156a9e9dc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-53b0620b-dc2c-4b34-bca1-35f156a9e9dc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-53b0620b-dc2c-4b34-bca1-35f156a9e9dc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":1}],"source":["import pandas as pd\n","\n","# Loading the spam data\n","# ham is the label for non-spam messages\n","spam = pd.read_csv('data.csv')\n","spam.head(10)"]},{"cell_type":"markdown","id":"189e875b","metadata":{"papermill":{"duration":0.014025,"end_time":"2021-11-09T00:09:34.653996","exception":false,"start_time":"2021-11-09T00:09:34.639971","status":"completed"},"tags":[],"id":"189e875b"},"source":["# Bag of Words\n","Machine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric.\n","\n","The simplest common representation is a variation of one-hot encoding. You represent each document as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents). \n","\n","As an example, take the sentences \"Tea is life. Tea is love.\" and \"Tea is healthy, calming, and delicious.\" as our corpus. The vocabulary then is `{\"tea\", \"is\", \"life\", \"love\", \"healthy\", \"calming\", \"and\", \"delicious\"}` (ignoring punctuation).\n","\n","For each document, count up how many times a term occurs, and place that count in the appropriate element of a vector. The first sentence has \"tea\" twice and that is the first position in our vocabulary, so we put the number 2 in the first element of the vector. Our sentences as vectors then look like \n","\n","$$\n","\\begin{align}\n","v_1 &= \\left[\\begin{matrix} 2 & 2 & 1 & 1 & 0 & 0 & 0 & 0 \\end{matrix}\\right] \\\\\n","v_2 &= \\left[\\begin{matrix} 1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\end{matrix}\\right]\n","\\end{align}\n","$$\n","\n","This is called the **bag of words** representation. You can see that documents with similar terms will have similar vectors. Vocabularies frequently have tens of thousands of terms, so these vectors can be very large.\n","\n","Another common representation is **TF-IDF (Term Frequency - Inverse Document Frequency)**. TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models. You won't need it here. Feel free to look it up though!"]},{"cell_type":"markdown","id":"aa0524e6","metadata":{"papermill":{"duration":0.014086,"end_time":"2021-11-09T00:09:34.682511","exception":false,"start_time":"2021-11-09T00:09:34.668425","status":"completed"},"tags":[],"id":"aa0524e6"},"source":["# Building a Bag of Words model\n","\n","Once you have your documents in a bag of words representation, you can use those vectors as input to any machine learning model. spaCy handles the bag of words conversion and building a simple linear model for you with the `TextCategorizer` class.\n","\n","The TextCategorizer is a spaCy **pipe**. Pipes are classes for processing and transforming tokens. When you create a spaCy model with `nlp = spacy.load('en_core_web_sm')`, there are default pipes that perform part of speech tagging, entity recognition, and other transformations. When you run text through a model `doc = nlp(\"Some text here\")`, the output of the pipes are attached to the tokens in the `doc` object. The lemmas for `token.lemma_` come from one of these pipes.\n","\n","You can remove or add pipes to models. What we'll do here is create an empty model without any pipes (other than a tokenizer, since all models always have a tokenizer). Then, we'll create a TextCategorizer pipe and add it to the empty model."]},{"cell_type":"markdown","source":["This code block initializes an empty spaCy model for the English language and adds a TextCategorizer component to it. The TextCategorizer component is used for text classification tasks.\n","\n","\n","\n"],"metadata":{"id":"BasMhUDGhDq5"},"id":"BasMhUDGhDq5"},{"cell_type":"code","execution_count":null,"id":"90838993","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:09:34.716593Z","iopub.status.busy":"2021-11-09T00:09:34.715510Z","iopub.status.idle":"2021-11-09T00:09:45.550712Z","shell.execute_reply":"2021-11-09T00:09:45.551275Z"},"papermill":{"duration":10.854553,"end_time":"2021-11-09T00:09:45.551451","exception":false,"start_time":"2021-11-09T00:09:34.696898","status":"completed"},"tags":[],"id":"90838993"},"outputs":[],"source":["import spacy\n","\n","# Create an empty model\n","nlp = spacy.blank(\"en\")\n","\n","# Add the TextCategorizer to the empty model\n","textcat = nlp.add_pipe(\"textcat\")"]},{"cell_type":"markdown","id":"98ee3729","metadata":{"papermill":{"duration":0.014142,"end_time":"2021-11-09T00:09:45.580035","exception":false,"start_time":"2021-11-09T00:09:45.565893","status":"completed"},"tags":[],"id":"98ee3729"},"source":["Next we'll add the labels to the model. Here \"ham\" are for the real messages, \"spam\" are spam messages."]},{"cell_type":"markdown","source":["This code block is adding two labels \"ham\" and \"spam\" to the text classifier in order to train the model to distinguish between these two categories of text.\n","\n","\n","\n"],"metadata":{"id":"JquNdCnfhHcT"},"id":"JquNdCnfhHcT"},{"cell_type":"code","execution_count":null,"id":"0e39a7b8","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:09:45.612754Z","iopub.status.busy":"2021-11-09T00:09:45.611772Z","iopub.status.idle":"2021-11-09T00:09:45.618346Z","shell.execute_reply":"2021-11-09T00:09:45.617836Z"},"papermill":{"duration":0.024073,"end_time":"2021-11-09T00:09:45.618488","exception":false,"start_time":"2021-11-09T00:09:45.594415","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"0e39a7b8","executionInfo":{"status":"ok","timestamp":1681716596755,"user_tz":-420,"elapsed":16,"user":{"displayName":"Tuấn Long Hồ","userId":"06884557049593639963"}},"outputId":"bfc0c20c-2eb6-4fc3-b439-43b3ae199992"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":3}],"source":["# Add labels to text classifier\n","textcat.add_label(\"ham\")\n","textcat.add_label(\"spam\")"]},{"cell_type":"markdown","id":"24901c08","metadata":{"papermill":{"duration":0.014595,"end_time":"2021-11-09T00:09:45.647983","exception":false,"start_time":"2021-11-09T00:09:45.633388","status":"completed"},"tags":[],"id":"24901c08"},"source":["# Training a Text Categorizer Model\n","\n","Next, you'll convert the labels in the data to the form TextCategorizer requires. For each document, you'll create a dictionary of boolean values for each class. \n","\n","For example, if a text is \"ham\", we need a dictionary `{'ham': True, 'spam': False}`. The model is looking for these labels inside another dictionary with the key `'cats'`."]},{"cell_type":"markdown","source":["This code block prepares the training data for a text classifier. The variable train_texts is a NumPy array that contains the text messages to be classified. The variable train_labels is a list of dictionaries where each dictionary represents the label for a text message. The label is specified as a binary value for each of the two categories (ham and spam).\n","\n","\n","\n"],"metadata":{"id":"qWlb0bfIhLyj"},"id":"qWlb0bfIhLyj"},{"cell_type":"code","execution_count":null,"id":"46883d1c","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:09:45.681531Z","iopub.status.busy":"2021-11-09T00:09:45.680520Z","iopub.status.idle":"2021-11-09T00:09:45.694989Z","shell.execute_reply":"2021-11-09T00:09:45.695575Z"},"papermill":{"duration":0.032987,"end_time":"2021-11-09T00:09:45.695749","exception":false,"start_time":"2021-11-09T00:09:45.662762","status":"completed"},"tags":[],"id":"46883d1c"},"outputs":[],"source":["train_texts = spam['text'].values\n","train_labels = [{'cats': {'ham': label == 'ham',\n","                          'spam': label == 'spam'}} \n","                for label in spam['label']]"]},{"cell_type":"markdown","id":"02f85189","metadata":{"papermill":{"duration":0.015048,"end_time":"2021-11-09T00:09:45.726731","exception":false,"start_time":"2021-11-09T00:09:45.711683","status":"completed"},"tags":[],"id":"02f85189"},"source":["Then we combine the texts and labels into a single list."]},{"cell_type":"markdown","source":["The code block creates a list of tuples train_data, where each tuple contains a text message and its corresponding label. The labels are in the form of a dictionary that has two boolean values indicating whether the message is spam or not (True if it is spam, False if it is ham). The first three tuples of the list are printed.\n","\n","\n","\n"],"metadata":{"id":"yzjrl_HxhQQU"},"id":"yzjrl_HxhQQU"},{"cell_type":"code","execution_count":null,"id":"8a61ff76","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:09:45.762339Z","iopub.status.busy":"2021-11-09T00:09:45.761084Z","iopub.status.idle":"2021-11-09T00:09:45.770293Z","shell.execute_reply":"2021-11-09T00:09:45.770879Z"},"papermill":{"duration":0.028358,"end_time":"2021-11-09T00:09:45.771058","exception":false,"start_time":"2021-11-09T00:09:45.742700","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"8a61ff76","executionInfo":{"status":"ok","timestamp":1681716607274,"user_tz":-420,"elapsed":16,"user":{"displayName":"Tuấn Long Hồ","userId":"06884557049593639963"}},"outputId":"e6d7c52d-81e8-42bd-f07b-b749b6e7e929"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n","  {'cats': {'ham': True, 'spam': False}}),\n"," ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n"," (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n","  {'cats': {'ham': False, 'spam': True}})]"]},"metadata":{},"execution_count":5}],"source":["train_data = list(zip(train_texts, train_labels))\n","train_data[:3]"]},{"cell_type":"markdown","id":"fd959d63","metadata":{"papermill":{"duration":0.01557,"end_time":"2021-11-09T00:09:45.802055","exception":false,"start_time":"2021-11-09T00:09:45.786485","status":"completed"},"tags":[],"id":"fd959d63"},"source":["Now you are ready to train the model. First, create an `optimizer` using `nlp.begin_training()`. spaCy uses this optimizer to update the model. In general it's more efficient to train models in small batches. spaCy provides the `minibatch` function that returns a generator yielding minibatches for training. Finally, the minibatches are split into texts and labels, then used with `nlp.update` to update the model's parameters."]},{"cell_type":"markdown","source":["This code block is performing text classification training using the Spacy library. It is creating batches of data, iterating through each batch, creating a document object from each text, creating a training example from each document and its corresponding label, and updating the model using the optimizer. The ultimate goal is to train the text classifier to predict whether a given text is \"ham\" or \"spam\".\n","\n","\n","\n"],"metadata":{"id":"kjB9C3gwhU1_"},"id":"kjB9C3gwhU1_"},{"cell_type":"code","execution_count":null,"id":"6ba132a4","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:09:45.837342Z","iopub.status.busy":"2021-11-09T00:09:45.836308Z","iopub.status.idle":"2021-11-09T00:11:14.644951Z","shell.execute_reply":"2021-11-09T00:11:14.645508Z"},"papermill":{"duration":88.827873,"end_time":"2021-11-09T00:11:14.645715","exception":false,"start_time":"2021-11-09T00:09:45.817842","status":"completed"},"tags":[],"id":"6ba132a4"},"outputs":[],"source":["from spacy.util import minibatch\n","from spacy.training.example import Example\n","\n","spacy.util.fix_random_seed(1)\n","optimizer = nlp.begin_training()\n","\n","# Create the batch generator with batch size = 8\n","batches = minibatch(train_data, size=8)\n","# Iterate through minibatches\n","for batch in batches:\n","    # Each batch is a list of (text, label) \n","    for text, labels in batch:\n","        doc = nlp.make_doc(text)\n","        example = Example.from_dict(doc, labels)\n","        nlp.update([example], sgd=optimizer)"]},{"cell_type":"markdown","id":"c664f236","metadata":{"papermill":{"duration":0.016182,"end_time":"2021-11-09T00:11:14.679306","exception":false,"start_time":"2021-11-09T00:11:14.663124","status":"completed"},"tags":[],"id":"c664f236"},"source":["This is just one training loop (or epoch) through the data. The model will typically need multiple epochs. Use another loop for more epochs, and optionally re-shuffle the training data at the begining of each loop. "]},{"cell_type":"markdown","source":["This code block trains a text classification model using spaCy's text categorizer. It initializes the model and adds labels, prepares training data and trains the model for 10 epochs using mini-batch gradient descent. It then prints the loss for each epoch.\n","\n","\n","\n"],"metadata":{"id":"PLAT9J4NhZHH"},"id":"PLAT9J4NhZHH"},{"cell_type":"code","execution_count":null,"id":"7ec7db6e","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:11:14.720779Z","iopub.status.busy":"2021-11-09T00:11:14.720085Z","iopub.status.idle":"2021-11-09T00:26:17.843137Z","shell.execute_reply":"2021-11-09T00:26:17.843736Z"},"papermill":{"duration":903.148076,"end_time":"2021-11-09T00:26:17.844005","exception":false,"start_time":"2021-11-09T00:11:14.695929","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"7ec7db6e","executionInfo":{"status":"ok","timestamp":1681717405760,"user_tz":-420,"elapsed":713432,"user":{"displayName":"Tuấn Long Hồ","userId":"06884557049593639963"}},"outputId":"6ca17c0b-8ded-4d97-85d7-8cdc96ede5ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'textcat': 126.98869873686914}\n","{'textcat': 186.98192019881125}\n","{'textcat': 224.27931302860458}\n","{'textcat': 252.536974326176}\n","{'textcat': 270.75695920388745}\n","{'textcat': 282.9079171721202}\n","{'textcat': 292.5052033042927}\n","{'textcat': 301.5247171347621}\n","{'textcat': 311.2253905344236}\n","{'textcat': 321.70840592757884}\n"]}],"source":["import random\n","\n","random.seed(1)\n","spacy.util.fix_random_seed(1)\n","optimizer = nlp.begin_training()\n","\n","losses = {}\n","for epoch in range(10):\n","    random.shuffle(train_data)\n","    # Create the batch generator with batch size = 8\n","    batches = minibatch(train_data, size=8)\n","    # Iterate through minibatches\n","    for batch in batches:\n","        for text, labels in batch:\n","            doc = nlp.make_doc(text)\n","            example = Example.from_dict(doc, labels)\n","            nlp.update([example], sgd=optimizer, losses=losses)\n","    print(losses)"]},{"cell_type":"markdown","id":"ef091b25","metadata":{"papermill":{"duration":0.020821,"end_time":"2021-11-09T00:26:17.885009","exception":false,"start_time":"2021-11-09T00:26:17.864188","status":"completed"},"tags":[],"id":"ef091b25"},"source":["# Making Predictions"]},{"cell_type":"markdown","id":"79ef02ef","metadata":{"papermill":{"duration":0.019895,"end_time":"2021-11-09T00:26:17.925196","exception":false,"start_time":"2021-11-09T00:26:17.905301","status":"completed"},"tags":[],"id":"79ef02ef"},"source":["Now that you have a trained model, you can make predictions with the `predict()` method. The input text needs to be tokenized with `nlp.tokenizer`. Then you pass the tokens to the predict method which returns scores. The scores are the probability the input text belongs to the classes."]},{"cell_type":"markdown","source":["This code block is using a trained spaCy model to classify two example text documents using a text categorizer. It first tokenizes the texts and stores them as spaCy documents. It then retrieves the 'textcat' component from the model and uses it to predict the category scores for each document. The predicted scores are printed to the console.\n","\n","\n","\n"],"metadata":{"id":"vItCDpcfhdDv"},"id":"vItCDpcfhdDv"},{"cell_type":"code","execution_count":null,"id":"8de07adf","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:26:17.980097Z","iopub.status.busy":"2021-11-09T00:26:17.978749Z","iopub.status.idle":"2021-11-09T00:26:17.986467Z","shell.execute_reply":"2021-11-09T00:26:17.987089Z"},"papermill":{"duration":0.041248,"end_time":"2021-11-09T00:26:17.987261","exception":false,"start_time":"2021-11-09T00:26:17.946013","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"8de07adf","executionInfo":{"status":"ok","timestamp":1681717414848,"user_tz":-420,"elapsed":823,"user":{"displayName":"Tuấn Long Hồ","userId":"06884557049593639963"}},"outputId":"f2198b96-173a-467c-e4e6-09e8d943768b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[9.999808e-01 1.913596e-05]\n"," [4.763503e-02 9.523650e-01]]\n"]}],"source":["texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n","         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n","docs = [nlp.tokenizer(text) for text in texts]\n","    \n","# Use textcat to get the scores for each doc\n","textcat = nlp.get_pipe('textcat')\n","scores = textcat.predict(docs)\n","\n","print(scores)"]},{"cell_type":"markdown","id":"853cdd8a","metadata":{"papermill":{"duration":0.020736,"end_time":"2021-11-09T00:26:18.028740","exception":false,"start_time":"2021-11-09T00:26:18.008004","status":"completed"},"tags":[],"id":"853cdd8a"},"source":["The scores are used to predict a single class or label by choosing the label with the highest probability. You get the index of the highest probability with `scores.argmax`, then use the index to get the label string from `textcat.labels`."]},{"cell_type":"markdown","source":["This code block takes the scores predicted by a text classification model using textcat.predict() on a list of documents, and then uses argmax() to find the label with the highest score/probability for each document. It then prints the predicted labels for each document.\n","\n","\n","\n"],"metadata":{"id":"ZrRA11Y_hhZf"},"id":"ZrRA11Y_hhZf"},{"cell_type":"code","execution_count":null,"id":"4a4fac77","metadata":{"execution":{"iopub.execute_input":"2021-11-09T00:26:18.073895Z","iopub.status.busy":"2021-11-09T00:26:18.072935Z","iopub.status.idle":"2021-11-09T00:26:18.078477Z","shell.execute_reply":"2021-11-09T00:26:18.079026Z"},"papermill":{"duration":0.029632,"end_time":"2021-11-09T00:26:18.079196","exception":false,"start_time":"2021-11-09T00:26:18.049564","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"4a4fac77","executionInfo":{"status":"ok","timestamp":1681717418436,"user_tz":-420,"elapsed":529,"user":{"displayName":"Tuấn Long Hồ","userId":"06884557049593639963"}},"outputId":"c0076507-adaf-49a6-9031-e1f3becd8ba2"},"outputs":[{"output_type":"stream","name":"stdout","text":["['ham', 'spam']\n"]}],"source":["# From the scores, find the label with the highest score/probability\n","predicted_labels = scores.argmax(axis=1)\n","print([textcat.labels[label] for label in predicted_labels])"]},{"cell_type":"markdown","id":"661b66cc","metadata":{"papermill":{"duration":0.020668,"end_time":"2021-11-09T00:26:18.121016","exception":false,"start_time":"2021-11-09T00:26:18.100348","status":"completed"},"tags":[],"id":"661b66cc"},"source":["Evaluating the model is straightforward once you have the predictions. To measure the accuracy, calculate how many correct predictions are made on some test data, divided by the total number of predictions."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":1017.28692,"end_time":"2021-11-09T00:26:21.794560","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-11-09T00:09:24.507640","version":"2.3.3"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}